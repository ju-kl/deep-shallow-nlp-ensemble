{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "import string\n",
    "from scipy import spatial\n",
    "from gensim.models import Word2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COVID Hackerthon: Word2Vec \n",
    "**Purpose of this notebook:** Identify specific characteristics of patients within a population cohort that may be the best candidates of a COVID-19 clinical trial using Word2Vec.\n",
    "\n",
    "Resources used:\n",
    "- https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92\n",
    "- https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/\n",
    "- https://stackabuse.com/implementing-word2vec-with-gensim-library-in-python/\n",
    "- https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(string):\n",
    "    \"\"\"\n",
    "    Takes a string (document) and tokanizes it.\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert into sentences\n",
    "    all_sentences = nltk.sent_tokenize(string)\n",
    "\n",
    "    # convert sentences into words\n",
    "    all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
    "\n",
    "    # remove punctuation\n",
    "    all_words_no_punc = [[word for word in all_word if word.isalnum()] for all_word in all_words]\n",
    "    \n",
    "    # return \n",
    "    return all_words_no_punc\n",
    "\n",
    "def cosine_sim(lst_1, lst_2):\n",
    "    \"\"\"\n",
    "    Takes two lists and calculates their cosine similarity.\n",
    "    \"\"\"\n",
    "        \n",
    "    return 1 - spatial.distance.cosine(lst_1, lst_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CLINICAL NOTES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>new COVID-19 patient confirmed in Switzerland:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>new COVID-19 patient confirmed in Croatia: mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1079</th>\n",
       "      <td>new COVID-19 patient confirmed in Algeria: mal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078</th>\n",
       "      <td>new COVID-19 patient confirmed in Afghanistan:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>new COVID-19 patient confirmed in Austria: 24,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         CLINICAL NOTES\n",
       "1081  new COVID-19 patient confirmed in Switzerland:...\n",
       "1080  new COVID-19 patient confirmed in Croatia: mal...\n",
       "1079  new COVID-19 patient confirmed in Algeria: mal...\n",
       "1078  new COVID-19 patient confirmed in Afghanistan:...\n",
       "1077  new COVID-19 patient confirmed in Austria: 24,..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "dat = pd.read_excel(\"Final_Hackathon_1082Patients.xlsx\", index_col=\"Unnamed: 0\")\n",
    "\n",
    "# head dat\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens in the doc is 1215.\n",
      "The total number of tokens in the doc is 1312946.\n"
     ]
    }
   ],
   "source": [
    "# find len of the docs\n",
    "len_doc = []\n",
    "for i in range(len(dat)):\n",
    "    len_doc.append(len(dat.iloc[i,0].split()))\n",
    "\n",
    "# average len\n",
    "print(\"Average number of tokens in the doc is {:.0f}.\".format(np.mean(len_doc)))\n",
    "\n",
    "# total tokes in the \n",
    "print(\"The total number of tokens in the doc is {:.0f}.\".format(np.sum(len_doc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to array\n",
    "arr = dat.values\n",
    "\n",
    "# all lower cases\n",
    "lst_prepro = []\n",
    "for i in arr:\n",
    "    lst_prepro.append(i[0].lower().replace(\"-\", \"\"))\n",
    "    \n",
    "# split() to find index on all words\n",
    "lst_prepro_split = []\n",
    "for i in lst_prepro:\n",
    "    lst_prepro_split.append(i.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec (own training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create one document to fit the model on\n",
    "doc_train = \" \".join(lst_prepro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize entire document\n",
    "all_words_no_punc = tokenization(doc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model with default parameters\n",
    "embedding = Word2Vec(all_words_no_punc, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the vocabulary is: 25211\n"
     ]
    }
   ],
   "source": [
    "# find vocabulary\n",
    "vocab = list(embedding.wv.vocab)\n",
    "print(\"Length of the vocabulary is:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('56yearold', 0.7957502603530884),\n",
       " ('gentleman', 0.7912894487380981),\n",
       " ('50yearold', 0.7822447419166565),\n",
       " ('62yearold', 0.7819993495941162),\n",
       " ('76yo', 0.7772549390792847),\n",
       " ('22f', 0.7675936222076416),\n",
       " ('woman', 0.7593116760253906),\n",
       " ('man', 0.7561774253845215),\n",
       " ('yearold', 0.7482136487960815),\n",
       " ('mmp', 0.7461156249046326)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find similar words\n",
    "embedding.wv.most_similar('bmi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the embedding is: 100\n",
      "[ 3.6991994e+00  2.9168355e-01 -2.7359939e+00 -2.5249883e-03\n",
      "  6.8504447e-01 -3.5621521e-01 -2.1205173e+00  2.8659716e+00\n",
      " -2.4683878e+00 -1.2167168e+00  1.0319304e+00 -2.7930064e+00\n",
      " -1.2197591e+00  2.3394346e+00  2.8453608e+00  1.5508924e-01\n",
      "  1.8451068e+00  5.2849402e+00  2.0767226e+00  4.3833299e+00\n",
      "  2.4888189e+00  2.6452329e+00 -2.2902064e+00  2.6005089e+00\n",
      " -8.7425131e-01 -4.4344461e-01  1.2826571e+00  4.6215571e-02\n",
      " -2.5944769e-01 -9.0982574e-01  2.3621571e+00  1.5400938e+00\n",
      "  2.4593534e+00 -2.8693128e+00  1.3186111e+00 -1.2558672e+00\n",
      "  1.4995192e+00 -1.6397364e-01  6.6479486e-01 -1.7692365e-02\n",
      "  3.4675066e+00  1.5277199e+00  7.5546846e-02  6.0425329e-01\n",
      "  3.8559443e-01 -1.3586041e+00 -2.3652210e+00 -2.4005497e+00\n",
      " -3.2230418e+00 -7.5790799e-01  4.0161591e+00  4.5248141e+00\n",
      "  3.3418289e-01 -3.3568916e-01 -1.5802268e+00 -3.7338775e-02\n",
      " -2.1363802e+00 -2.9060760e+00 -4.0148187e-01 -3.1622691e+00\n",
      "  4.1173381e-01  4.3294840e+00 -1.0095180e-01  8.3966613e-01\n",
      "  9.3827508e-02  4.2462277e+00 -1.0442246e+00  4.1114051e-02\n",
      "  4.4197841e+00 -9.7109580e-01 -2.1993287e+00  1.5165251e-01\n",
      " -3.1158975e-01 -2.3320777e+00 -1.0013546e-01 -3.5343799e-01\n",
      " -2.5888114e+00  4.5731139e+00  2.5375209e+00 -1.3273116e+00\n",
      " -1.2314843e-01 -1.4878869e-02 -3.3013802e+00 -2.1248517e+00\n",
      "  2.2454987e+00 -2.3420050e+00  3.6183453e+00 -2.3656130e+00\n",
      " -4.5148225e+00  7.8318471e-01  3.8680968e+00 -2.9993601e+00\n",
      "  6.8032199e-01 -8.2364684e-01 -1.4572147e-02 -2.2958288e+00\n",
      "  5.6154037e+00  2.6604750e+00  3.7215195e+00  2.4253850e+00]\n"
     ]
    }
   ],
   "source": [
    "# embedding for a word\n",
    "tst = embedding.wv.__getitem__(\"covid19\")\n",
    "print(\"Length of the embedding is:\", len(tst))\n",
    "print(tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary with embeddings\n",
    "embedding_dct = {}\n",
    "for i in vocab:\n",
    "    embedding_dct[i]=list(embedding.wv.__getitem__(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find relevant section in original documents\n",
    "- The relevnat section will be the sentence that includes the word with the highest cosine similarity to the defined search word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - find the cosine similarity for each word in the document to the search terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip punctuation for all documents\n",
    "lst_prepro_split_clean = []\n",
    "\n",
    "for i in range(len(lst_prepro_split)):\n",
    "    lst_prepro_split_clean.append([i.replace(\".\", \"\").replace(\",\", \"\").replace(\":\", \"\") for i in lst_prepro_split[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedded representation for all documents\n",
    "\n",
    "doc_embedded = []\n",
    "\n",
    "# loop over all documents\n",
    "for i in range(len(lst_prepro_split_clean)):\n",
    "    # initialize temp lst for each document \n",
    "    temp = []\n",
    "    # loop over each token in the document\n",
    "    for token in lst_prepro_split_clean[i]:\n",
    "        # try to find the respective embedding\n",
    "        try:\n",
    "            temp.append(embedding_dct[token])\n",
    "        # cant find embedding, this happens because data preprocessing differs, append embedding of 0 (cosine similarity to that will be nan)\n",
    "        except:\n",
    "            temp.append(0)\n",
    "    # append temp to doc_embedded\n",
    "    doc_embedded.append(temp)\n",
    "\n",
    "# error handling\n",
    "if len(lst_prepro_split_clean) != len(doc_embedded):\n",
    "    print(\"Error, lists have different lengths\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search list\n",
    "search_list = ['positive', 'covid19', 'age', 'year', 'female','male', 'man', 'woman', 'bmi','kg', 'consent','agree', 'allergies', 'hiv', 'immunodeficiency', \n",
    "               'virus', 'hepatitis', 'chronic', 'hepatic', 'drug', 'opioids', \"alt\", \"tall\", \"feet\", \"height\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over all search terms to find the cosine similarity between each word in the document and the search term\n",
    "for search_term in search_list:\n",
    "    \n",
    "    #pnitor process\n",
    "    print(search_term)\n",
    "    \n",
    "    # define search vector\n",
    "    search = embedding_dct[search_term]\n",
    "\n",
    "    # find cosine similarities of all words to the search term\n",
    "    cosine_similarity = []\n",
    "\n",
    "    # loop over all documents\n",
    "    for i in range(len(doc_embedded)):\n",
    "        # monitor process\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "        # loop over each token in the document\n",
    "        temp = [cosine_sim(search, token) for token in doc_embedded[i]]\n",
    "        # replace nan with 0\n",
    "        temp = [0 if math.isnan(i) else i for i in temp]\n",
    "        # append temp to cosine_similarity\n",
    "        cosine_similarity.append(temp)   \n",
    "\n",
    "    # error handling\n",
    "    if len(cosine_similarity) != len(doc_embedded):\n",
    "        print(\"Error, something is wrong\", search_term)\n",
    "\n",
    "    # save dataframe (row - patient, colum - word index)\n",
    "    pd.DataFrame(cosine_similarity).to_csv(\"data/\" + str(search_term) + \".csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - find the phrase (+/- n words around the keyword) for a search criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenizer (format we used across the different approaches)\n",
    "\n",
    "# load data\n",
    "raw = pd.read_excel(\"Final_Hackathon_1082Patients.xlsx\")\n",
    "\n",
    "# name columns\n",
    "raw.columns = ['id','note']\n",
    "\n",
    "sentences = []\n",
    "for i in range(len(raw['note'])):\n",
    "    sentences.append(nltk.tokenize.sent_tokenize(raw['note'][i]))\n",
    "\n",
    "# add to df \n",
    "raw['sent'] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define threshold for the cosine similarity\n",
    "thres = 0.9\n",
    "\n",
    "# loop over all search terms: create column with index of the sentence that includes keywords\n",
    "for search_term in search_list:\n",
    "    \n",
    "    # load data\n",
    "    df = pd.read_csv(\"data/\" + str(search_term) + \".csv\", index_col = \"Unnamed: 0\")\n",
    "\n",
    "    # fill na with 0\n",
    "    df = df.fillna(0).T\n",
    "\n",
    "    # find indixes of the words most similar to the seach word\n",
    "    search_idx = []\n",
    "\n",
    "    # for each document (columns of the df)\n",
    "    for document in range(df.shape[1]):\n",
    "\n",
    "        # find words with a cosine similarity to the search word higher than the thres\n",
    "        temp = df[document] > thres\n",
    "\n",
    "        # save indices of the words (index of the series)\n",
    "        temp = [float(i) for i in list(temp[temp == True].index)]\n",
    "\n",
    "        # append indixes of search word to list\n",
    "        search_idx.append(temp)  \n",
    "\n",
    "    # define range\n",
    "    word_range = 4\n",
    "\n",
    "    # find phrases for each keyword\n",
    "    phrases = []\n",
    "    # loop over each patient\n",
    "    for patient in range(len(lst_prepro_split_clean)):\n",
    "        temp = []\n",
    "        # for each keyword in the document\n",
    "        for keyword_idx in search_idx[patient]:\n",
    "            # append the phrase with the offset of the range around the keyword\n",
    "            temp.append(\" \".join(lst_prepro_split_clean[patient][max(0, int(keyword_idx-word_range)):min(int(keyword_idx+word_range), len(lst_prepro_split_clean[patient]))]))\n",
    "        # append temp\n",
    "        phrases.append(temp)\n",
    "\n",
    "    # create column for criteria \n",
    "    raw[str(search_term)] = phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criteria\n",
    "covid_criteria = ['positive','covid19']\n",
    "age_criteria = ['age', 'year']\n",
    "sex_criteria = ['female','male', 'man', 'woman']\n",
    "consent_criteria = ['consent','agree']\n",
    "bmi_criteria = ['bmi']\n",
    "weight_criteria = ['kg']\n",
    "height_criteria = [\"tall\", \"feet\", \"height\"]\n",
    "allergies_criteria = ['allergies', 'drug']\n",
    "hiv_criteria = ['hiv', 'immunodeficiency', 'virus']\n",
    "hepatitis_criteria = ['hepatitis', 'chronic', 'hepatic']\n",
    "alt_asn_criteria = [\"alt\"]\n",
    "opioids_criteria = ['opioids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create final df\n",
    "temp = raw[[\"id\", \"note\", \"sent\"]].copy()\n",
    "temp[\"c1_word\"] = raw[\"positive\"] + raw[\"covid19\"]\n",
    "temp[\"c2_word\"] = raw[\"age\"] + raw[\"year\"]\n",
    "temp[\"c3_word\"] = raw[\"female\"] + raw[\"male\"] + raw[\"man\"] + raw[\"woman\"]\n",
    "temp[\"c4_word\"] = raw[\"consent\"] + raw[\"agree\"]\n",
    "temp[\"c5_word\"] = raw[\"bmi\"]\n",
    "temp[\"c6_word\"] = raw[\"kg\"]\n",
    "temp[\"c7_word\"] = raw[\"tall\"] + raw[\"feet\"] + raw[\"height\"]\n",
    "temp[\"c8_word\"] = raw[\"allergies\"] + raw[\"allergies\"]\n",
    "temp[\"c9_word\"] = raw[\"hiv\"] + raw[\"immunodeficiency\"] + raw[\"virus\"]\n",
    "temp[\"c10_word\"] = raw[\"hepatitis\"] + raw[\"hepatic\"]\n",
    "temp[\"c11_word\"] = raw[\"alt\"] \n",
    "temp[\"c12_word\"] = raw[\"opioids\"]\n",
    "\n",
    "temp.drop(columns=[\"note\", \"sent\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>c1_word</th>\n",
       "      <th>c2_word</th>\n",
       "      <th>c3_word</th>\n",
       "      <th>c4_word</th>\n",
       "      <th>c5_word</th>\n",
       "      <th>c6_word</th>\n",
       "      <th>c7_word</th>\n",
       "      <th>c8_word</th>\n",
       "      <th>c9_word</th>\n",
       "      <th>c10_word</th>\n",
       "      <th>c11_word</th>\n",
       "      <th>c12_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1081</td>\n",
       "      <td>[biopsy came back as positive for adenocarcino...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[patient confirmed in switzerland male 70 infe...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[vena cava and the hepatic artery in addition]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1080</td>\n",
       "      <td>[new covid19 patient confirmed in, new covid19...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[patient confirmed in croatia male recently re...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[train and dragged ~200 feet on  he]</td>\n",
       "      <td>[from 2/192/21 service surgery allergies clind...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1079</td>\n",
       "      <td>[new covid19 patient confirmed in, new covid19...</td>\n",
       "      <td>[at age 25 s/p tah at age 32, age 32 5mm nodul...</td>\n",
       "      <td>[patient confirmed in algeria male italian who...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[on 2/17 service medicine allergies codeine / ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1078</td>\n",
       "      <td>[new covid19 patient confirmed in, new covid19...</td>\n",
       "      <td>[age 25 s/p tah at age 32 social, 32 social hi...</td>\n",
       "      <td>[a 65 year old female with copd on, secondary ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[from qom service medicine allergies codeine /...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[fev1 6 (35%) fev1/fvc 29 (41%); on advair]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1077</td>\n",
       "      <td>[magnesium hbsab is positive the patient was o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[patient is a 57yearold female with hepatitis ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[lopressor ranitidine aldactone interferon all...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[innsbruck service admission diagnosis hepatit...</td>\n",
       "      <td>[bun 21 and creatinine alt is 349 ast, creatin...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                            c1_word  \\\n",
       "0  1081  [biopsy came back as positive for adenocarcino...   \n",
       "1  1080  [new covid19 patient confirmed in, new covid19...   \n",
       "2  1079  [new covid19 patient confirmed in, new covid19...   \n",
       "3  1078  [new covid19 patient confirmed in, new covid19...   \n",
       "4  1077  [magnesium hbsab is positive the patient was o...   \n",
       "\n",
       "                                             c2_word  \\\n",
       "0                                                 []   \n",
       "1                                                 []   \n",
       "2  [at age 25 s/p tah at age 32, age 32 5mm nodul...   \n",
       "3  [age 25 s/p tah at age 32 social, 32 social hi...   \n",
       "4                                                 []   \n",
       "\n",
       "                                             c3_word c4_word c5_word c6_word  \\\n",
       "0  [patient confirmed in switzerland male 70 infe...      []      []      []   \n",
       "1  [patient confirmed in croatia male recently re...      []      []      []   \n",
       "2  [patient confirmed in algeria male italian who...      []      []      []   \n",
       "3  [a 65 year old female with copd on, secondary ...      []      []      []   \n",
       "4  [patient is a 57yearold female with hepatitis ...      []      []      []   \n",
       "\n",
       "                                c7_word  \\\n",
       "0                                    []   \n",
       "1  [train and dragged ~200 feet on  he]   \n",
       "2                                    []   \n",
       "3                                    []   \n",
       "4                                    []   \n",
       "\n",
       "                                             c8_word c9_word  \\\n",
       "0                                                 []      []   \n",
       "1  [from 2/192/21 service surgery allergies clind...      []   \n",
       "2  [on 2/17 service medicine allergies codeine / ...      []   \n",
       "3  [from qom service medicine allergies codeine /...      []   \n",
       "4  [lopressor ranitidine aldactone interferon all...      []   \n",
       "\n",
       "                                            c10_word  \\\n",
       "0     [vena cava and the hepatic artery in addition]   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4  [innsbruck service admission diagnosis hepatit...   \n",
       "\n",
       "                                            c11_word c12_word  \n",
       "0                                                 []       []  \n",
       "1                                                 []       []  \n",
       "2                                                 []       []  \n",
       "3        [fev1 6 (35%) fev1/fvc 29 (41%); on advair]       []  \n",
       "4  [bun 21 and creatinine alt is 349 ast, creatin...       []  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('w2v_results.pkl', 'wb') as f:\n",
    "    pickle.dump(temp, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "temp.to_csv(\"w2v_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas for future work: \n",
    "- Use pre-trained word embeddings such as Word2Vec by Google or GloVe by Stanford"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
